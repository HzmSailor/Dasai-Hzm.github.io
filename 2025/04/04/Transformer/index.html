<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
<script>
MathJax = {
  tex: {
    inlineMath: [['$','$'], ['\\(','\\)']],  // 配置行内公式分隔符
    displayMath: [['$$','$$'], ['\\[','\\]']] // 配置行间公式分隔符
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // 避免处理代码块
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif Simplified Chinese:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dasai-hzm.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="TransformerTransformer 架构由 Google 在 NIPS 2017 提出，之后成为各种大模型的基础架构。一个完整的 Transformer 由 Encoder 和 Decoder 组成，整个架构图如下： 首先，整体概述 Transformer 的流程： 输入序列首先作 Input Embedding &amp; Positional Embedding, 映射为矩阵，之后经">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning: Transformer and Vision Transformer">
<meta property="og:url" content="https://dasai-hzm.github.io/2025/04/04/Transformer/index.html">
<meta property="og:site_name" content="HzmSailor&#39;s Learning Blog">
<meta property="og:description" content="TransformerTransformer 架构由 Google 在 NIPS 2017 提出，之后成为各种大模型的基础架构。一个完整的 Transformer 由 Encoder 和 Decoder 组成，整个架构图如下： 首先，整体概述 Transformer 的流程： 输入序列首先作 Input Embedding &amp; Positional Embedding, 映射为矩阵，之后经">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dasai-hzm.github.io/images/image-441.png">
<meta property="og:image" content="https://dasai-hzm.github.io/images/image-442.png">
<meta property="og:image" content="https://dasai-hzm.github.io/images/image-461.png">
<meta property="article:published_time" content="2025-04-04T14:48:16.000Z">
<meta property="article:modified_time" content="2025-04-06T13:56:15.573Z">
<meta property="article:author" content="HzmSailor">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dasai-hzm.github.io/images/image-441.png">

<link rel="canonical" href="https://dasai-hzm.github.io/2025/04/04/Transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Deep Learning: Transformer and Vision Transformer | HzmSailor's Learning Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">HzmSailor's Learning Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">HzmSailor 的技术博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/04/04/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning: Transformer and Vision Transformer
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-04 22:48:16" itemprop="dateCreated datePublished" datetime="2025-04-04T22:48:16+08:00">2025-04-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-06 21:56:15" itemprop="dateModified" datetime="2025-04-06T21:56:15+08:00">2025-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer 架构由 Google 在 NIPS 2017 提出，之后成为各种大模型的基础架构。一个完整的 Transformer 由 Encoder 和 Decoder 组成，整个架构图如下：<br><img src="\images\image-441.png" alt="alt text"></p>
<p>首先，整体概述 Transformer 的流程：</p>
<p>输入序列首先作 Input Embedding &amp; Positional Embedding, 映射为矩阵，之后经过 $N$ 个 Encoder 子模块，每个模块都由 Self-Attention 和 Feed Forward Network 组成，并且每个模块后都进行残差连接和 LayerNorm. 最后得到整个 Encoder 模块的输出矩阵 $X$, 以待 Decoder 中 Cross-Attention 模块使用。</p>
<p>将已经输出的序列作 Output Embedding &amp; Positioinal Embedding, 映射为矩阵，之后经过 $N$ 个 Decoder 子模块，每个模块都有一个 Masked-Attention, 一个 Cross-Attention, 一个 Feed Forward Attention 组成。同样每个模块之后都有残差连接和 LayerNorm.</p>
<p>最后解码器输出的结果经过线性映射，再经过 Softmax 映射为概率。从概率最大的几个 token 中按概率抽样，转为文本，输出。（取决于 top_p, temperature 等参数）</p>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><h4 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h4><p>由于 Transformer 最早应用于机器翻译领域，这里以机器翻译为示例。输入是一长段文本，首先要进行 tokenize, 就是把一长串文本化为 token, 每一个 token 可能是一个单词，也可能长于一个单词（如固定搭配表示特殊的语义），也可能短于一个单词（如词根词缀也能表示一定的独立语义）。</p>
<p>Tokenize 之后，要对 token 进行 embedding, 就是把 token 映射到向量。这一步可以基于一个词表做，例如 Word2Vec. 不过训练较为复杂的模型时，初始的 embedding 词表其实并不重要，随机初始化的效果与基于 Word2Vec 的效果差别不大。</p>
<h4 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h4><p>由于 Transformer 是把一串文本打散了之后输入，这样就会丢失某一个词在原始序列中的位置信息，因此在 Input Embedding 之后要做一步 Positional Embedding, 把初始的位置信息融入到嵌入向量中。具体公式如下：t 就是 token 在序列中的位置，i 表示 Positional Embedding 的第 i 维，H 表示 Embedding 的维数。</p>
<p><img src="\images\image-442.png" alt="alt text"></p>
<p>这样生成 Positional Embedding $p_k$ 与 Input Embedding $v_t$ 直接相加，得到总的 Embedding $x_t$. 这样输入从文本就转为一个矩阵 $x = [x_1, x_2,…, x_n]^T $.</p>
<h3 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h3><p>残差连接，就是把这一层的输出再加上这一层的输入。它旨在缓解梯度消失的问题，使得信息直接跨模块传递。</p>
<p>在梯度反向传播时，对于某一层的参数的偏导数，由于链式法则，是一系列偏导的乘积。连乘容易出现数值很小的情况，从而使参数更新缓慢，这一现象就是梯度消失。有了残差，求偏导就会多一个 1, 这样就避免了数值接近 0 的情况。</p>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="Methods-of-Normalization"><a href="#Methods-of-Normalization" class="headerlink" title="Methods of Normalization"></a>Methods of Normalization</h4><p>归一化的方法有很多，常见的有 BatchNorm, LayerNorm, RMSNorm, DeepNorm. </p>
<ul>
<li>BatchNorm (BN): 批次归一化，对于每次训练的 Batch，求得每一个维度的均值和标准差，进行缩放。然而对于小批量数据，这种用一个 Batch 的均值和标准差作为整体标准进行处理很容易误差较大，效果不好（NLP 任务中并不常用）。<br>\[<br>BatchNorm(x_i) = \frac{x_i - \mu_i}{\sigma_i} \cdot \gamma_i + \beta_i<br>\]</li>
</ul>
<p>\[<br>\mu_i = \frac{\sum_j x_i^{(j)}}{BatchSize}, \sigma = \sqrt{\frac{\sum_j (x_i^{(j)} - \mu_i)^2}{BatchSize}}<br>\]</p>
<ul>
<li>LayerNorm (LN): 层归一化，对于每一个数据，求其所有维度数据的均值和方差进行缩放。这样每一个数据的缩放仅与自己有关，解决了 BN 在小样本上效果不佳的问题。</li>
</ul>
<p>\[<br>LayerNorm(x) = \frac{x - \mu}{\sigma} \cdot \gamma + \beta<br>\]</p>
<p>\[<br>\mu = \frac{\sum x_i}{H}, \sigma = \sqrt{\frac{\sum (x_i - \mu)^2}{H}}<br>\]</p>
<ul>
<li>RMSNorm: RMS 归一化，可以看作是 Layer Norm 的变种，主要针对于 LN 要计算均值和标准差，计算量较大的问题。RMSNorm 直接令样本中每一个维度除以样本所有维度的方均值再缩放，RMSNorm 训练的速度和效果都比 LayerNorm 要好。</li>
</ul>
<p>\[<br>RMSNorm(x) = \frac{x}{\sqrt{\sum x_i^2} } \cdot \gamma<br>\]</p>
<ul>
<li><p>DeepNorm: 深度归一化，是一种 Post-Norm 方法，其实就是在残差连接中对之前的激活值 x 进行了缩放，残差计算之后再用 LayerNorm. MicroSoft 的研究人员发现这样的归一化可以使 Transformer 的训练更稳定，训练 1000 层以上的神经网络不再困难。</p>
<p>\[<br>DeepNorm(x) = LayerNorm( \alpha x + Sublayer(x))<br>\]</p>
</li>
</ul>
<p>注：可以发现在 BN, LN, RMSNorm 中都有一些 learnable 参数（如 $\gamma, \beta$），这可以调整归一化的强度，未必彻底的归一化才适合。特别的，当 $\gamma = \sigma$, $\beta = \mu$ 时，相当于并没有归一化。</p>
<h4 id="Position-of-NormLayer"><a href="#Position-of-NormLayer" class="headerlink" title="Position of NormLayer"></a>Position of NormLayer</h4><p>归一化的位置也有很多种不同的选择，总结如下，其中每种归一化的优劣引用了《大语言模型》（赵鑫等）的总结：</p>
<ul>
<li><p>Post-Norm: 后向归一化，最原始的Transformer中选择的是 Post-Norm，即一个 Sublayer(如 Attention, FeedForward) 之后做残差计算，残差之后再做归一化。</p>
<p>\[<br>PostNorm(x) = Norm(x + Sublayer(x))<br>\]<br>“在原理上，后向归一化具有很多优势。首先，有助于加快神经网络的训练收敛速度，使模型可以更有效地传播梯度，从而减少训练时间。其次，后向归一化可以降低神经网络对于超参数（如学习率、初始化参数等）的敏感性，使得网络更容易调优，并减少了超参数调整的难度。然而，由于在输出层附近存在梯度较大的问题，采用 Post-Norm 的 Transformer 模型在训练过程中通常会出现不稳定的现象。”</p>
</li>
<li><p>Pre-Norm: 前向归一化，对 Sublayer 的输入做归一化，之后再过 Sublayer 层，算残差。</p>
<p>\[<br>PreNorm(x) = x + Sublayer(Norm(x))<br>\]</p>
<p>“相较于 Post-Norm，Pre-Norm 直接把每个子层加在了归一化模块之后，仅仅对输入的表示进行了归一化，从而可以防止模型的梯度爆炸或者梯度消失现象。虽然使用了 Pre-Norm 的 Transformer 模型在训练过程中更加稳定，但是性能却逊色于采用了 Post-Norm 的模型。”</p>
</li>
<li><p>Sandwich-Norm: 三明治归一化，就是 Pre-Norm 和 Post-Norm 的结合，虽然想法比较理想，从理论上应该性能更好，但是实际中研究人员发现这种办法并不能使训练完全稳定，有时候甚至会崩溃。<br>\[<br>SandwichNorm(x) = x + Norm(Sublayer(Norm(x)))<br>\]</p>
</li>
</ul>
<h3 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h3><p>前馈神经网络，就是信息流动始终由前一层流向后一层，各层间信息没有反馈。比如 CNN, MLP 都属于前馈神经网络，而 RNN 不是。原始的 Transformer 这里用的是两层神经网络（两层的 MLP）。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attention 机制是 Transformer 的独创性的重大突破，直接建模了任意距离的词元之间的语义关联，实现了在上下文中理解语义信息（解决了一词多义等问题），从而提高了模型理解复杂文本的能力。</p>
<h4 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h4><p>Multi-head Attention 计算过程：<br>先将输入矩阵 $X$ 映射为 $Q, K, V$:</p>
<p>\[<br>Q = X W^Q<br>\]</p>
<p>\[<br>K = X W^K<br>\]</p>
<p>\[<br>V = X W^V<br>\]</p>
<p>然后计算 Attention:</p>
<p>\[<br>Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{D}})V<br>\]</p>
<p>注意在 Decoder 中的 Attention 是 Masked-Attetion, 即要在计算 Attention 之前，把上三角元全部设为 -inf，这是因为使用词元预测的方法训练时，要使 Decoder 只能使用当前已生成的信息，而不能使用“未知”的信息。</p>
<p>在 Multi-head Attention 中，有 $N$ 组结构相同，而参数不同的 $W^Q, W^K, W^V$ 映射矩阵，把每一组参数作 Attention 之后的结果拼接起来，再通过一个矩阵映射得到最终的结果。</p>
<p>\[<br>head_i = Attention(XW^Q_i, XW^K_i, XW^V_i)<br>\]</p>
<p>\[<br>MHA(X) = Concat(head_1, head_2,…,head_n) W^O<br>\]</p>
<p><img src="\images\image-461.png" alt="alt text"></p>
<p>分析与解释：</p>
<ol>
<li>$K, Q, V$ 各自的含义？<br>$Q$ 表示当前查询的位置，$K$ 表示被查询的其他位置，$V$ 表示实际的语义信息。用 $Q$ 与 $K$ 点乘计算当前查询的位置与其他位置的相似度，再用 Softmax 转为概率，最后乘 $V$, 其实是各个位置语义信息作加权平均。</li>
<li>为什么用多头注意力？<br>（1）允许模型再不同位置联合处理表达不同子空间信息能力，增强非线性表达能力（将输入向量投影到不同的子空间中，每个子空间可能对应不同任务/特征，比如翻译任务中，有的头关注主谓一致，有的头关注动宾搭配，有的头关注主句和从句的关系等等）；<br>（2）缓解秩塌缩问题（有论文推导 Transformer 的秩表达式，多次注意力层堆叠后，注意力矩阵的秩逐渐降低至1，模型的表达能力退化，趋于固定分布），多头注意力可以将注意力矩阵的秩直接扩大 $N$ 倍，一定程度上可以缓解秩塌缩的问题。</li>
<li>Attention 计算为什么除以 $\sqrt{D}$ ？<br>点积计算中数据随向量维度增大而增大，容易处在 Softmax 饱和区，梯度较小不易训练。</li>
</ol>
<h4 id="Self-Attention-amp-Cross-Attention"><a href="#Self-Attention-amp-Cross-Attention" class="headerlink" title="Self-Attention &amp; Cross-Attention"></a>Self-Attention &amp; Cross-Attention</h4><p>Self-Attention, 自注意力，用于建模同一模态信息内部之间的关联，有助于理解文本的上下文语义。Self-Attention 中的 $K, Q, V$ 全部来自于 Input/Output.</p>
<p>Cross-Attention, 交互注意力，用于实现跨模态之间的语义对齐。Cross Attention 用在解码器中，$Q$ 来自于 Output, 而 $K,V$ 来自于 Input.</p>
<p>在 Decoder 中，每一个子模块先包含了 Masked-Self Attention, 这一模块用于理解已输出的信息，而此模块后续的 Cross-Attention 用于理解未输出的原始模态的信息（例如翻译任务，Masked-Self Attention 关注”It is a”, Cross Attention 关注“猫”）.</p>
<p>未完待续</p>
<h2 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h2><p>Vision Transformer 旨在直接利用 Transformer 处理计算机视觉的任务，而不依赖于传统的 CNN。ViT 的主要思想是把一张图片打成 patch, 每一个 patch 相当于 NLP 任务中的一个 token. 这样就直接照搬 Transformer 中的 embedding, positional encoding, attention 等模块处理。</p>
<p>未完待续。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/03/21/CameraModel/" rel="prev" title="3D Vision: Camera Model and Calibration">
      <i class="fa fa-chevron-left"></i> 3D Vision: Camera Model and Calibration
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/06/30/summary-2025-1/" rel="next" title="大一下学期总结">
      大一下学期总结 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">1.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-number">1.1.</span> <span class="nav-text">Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Input-Embedding"><span class="nav-number">1.1.1.</span> <span class="nav-text">Input Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Positional-Embedding"><span class="nav-number">1.1.2.</span> <span class="nav-text">Positional Embedding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-Connection"><span class="nav-number">1.2.</span> <span class="nav-text">Residual Connection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalization"><span class="nav-number">1.3.</span> <span class="nav-text">Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Methods-of-Normalization"><span class="nav-number">1.3.1.</span> <span class="nav-text">Methods of Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Position-of-NormLayer"><span class="nav-number">1.3.2.</span> <span class="nav-text">Position of NormLayer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feed-Forward-Neural-Network"><span class="nav-number">1.4.</span> <span class="nav-text">Feed Forward Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-number">1.5.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-head-Attention"><span class="nav-number">1.5.1.</span> <span class="nav-text">Multi-head Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention-amp-Cross-Attention"><span class="nav-number">1.5.2.</span> <span class="nav-text">Self-Attention &amp; Cross-Attention</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vision-Transformer"><span class="nav-number">2.</span> <span class="nav-text">Vision Transformer</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HzmSailor"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">HzmSailor</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Dasai-Hzm" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Dasai-Hzm" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zimuhan276@gmail.com" title="E-Mail → mailto:zimuhan276@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      导航
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="/about/%20%7C%7C%20fa%20fa-user" title="&#x2F;about&#x2F; || fa fa-user">关于</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="/resources/%20%7C%7C%20fa%20fa-download" title="&#x2F;resources&#x2F; || fa fa-download">资源</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://xjtu-ai.github.io/" title="http:&#x2F;&#x2F;xjtu-ai.github.io" rel="noopener" target="_blank">XJTU-AI学组</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HzmSailor</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
